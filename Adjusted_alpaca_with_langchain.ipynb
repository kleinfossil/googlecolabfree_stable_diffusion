{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kleinfossil/googlecolabfree_stable_diffusion/blob/main/Adjusted_alpaca_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Inital Setup\n",
        "#@markdown This installs requirements and downloads the models\n",
        "Basemodel = \"decapoda-research/llama-7b-hf\" #@param [\"decapoda-research/llama-7b-hf\"]\n",
        "LoRa = \"tloen/alpaca-lora-7b\" #@param [\"tloen/alpaca-lora-7b\"]\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -q  git+https://github.com/huggingface/transformers\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets loralib sentencepiece \n",
        "!pip -q install bitsandbytes accelerate\n",
        "!pip -q install langchain\n",
        "clear_output()\n",
        "\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(Basemodel)\n",
        "lora_weights = LoRa\n",
        "\n",
        "base_model = LlamaForCausalLM.from_pretrained(\n",
        "    Basemodel,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        ")\n",
        "base_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_weights,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        "    )\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Tz9YQS5vtVYW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Setup Pipeline\n",
        "#@markdown This is somehow not working correctly. Need to check how to change it. \n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=base_model, \n",
        "    tokenizer=tokenizer, \n",
        "    max_length=1024,\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ZhpL3X25xM7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Chat without Memory\n",
        "#@markdown This will not use a memory\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction: \n",
        "{instruction}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
        "clear_output()\n",
        "llm_chain = LLMChain(prompt=prompt, \n",
        "                     llm=local_llm\n",
        "                     )"
      ],
      "metadata": {
        "id": "Jm8faJttyJoc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "singel_prompt = \"Tell me a joke, but not a knock-knock joke!\" #@param {type:\"string\"}\n",
        "\n",
        "print(llm_chain.run(singel_prompt))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k9-TEHtaxCbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Chat with memory\n",
        "#@markdown This will setup a chat with memory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# We are going to set the memory to go back 4 turns\n",
        "memory = 4 #@param {type:\"integer\"}\n",
        "window_memory = ConversationBufferWindowMemory(k=memory)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=local_llm, \n",
        "    memory=window_memory,\n",
        "    verbose=True\n",
        ")\n",
        "conversation.prompt.template\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "qVAhoCbH3SG0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Prompt = \"Can you make the story funny? \" #@param {type:\"string\"}\n",
        "conversation.predict(input=Prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "IDi4-gbP4YKi",
        "outputId": "7293ec04-5d1a-4341-83b0-037ab8c08fa0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me a happy story with 50 words. \n",
            "AI:  Once upon a time there was a little girl who loved playing in her garden. She would spend hours digging up worms and planting seeds. One day she found a special seed that grew into a beautiful flower. Everyone admired how pretty it looked so they decided to make it their own. They all took turns caring for this magical flower until one day it bloomed! It filled everyone's hearts with joy as it spread happiness throughout the town.\n",
            "Human: Can you make the story funny? \n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Once upon a time there was a little boy named Bob who really wanted to be a pirate but he had no idea what being a real life pirate entailed. So he went on a quest to find out more about them. He read books, watched movies, and even tried to dress like one. But nothing seemed right. Until finally his mom said \"Why don’t we just go visit some actual pirates?\" And off they set sail to meet these mysterious people. When they arrived at their destination, Bob was surprised by how much smaller than expected they were. In fact, they weren\\'t very impressive looking at all. Still determined, Bob asked if he could join them on their next voyage. To which they replied “Sure!” Little did he know that this journey would change him forever...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}